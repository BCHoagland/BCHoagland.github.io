<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,700|Source+Sans+Pro:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="styling/style.css">
    <title>Braden Hoagland</title>
</head>
<body>

<div class="row">
    <!-- IMAGE -->
    <div class="col-lg-5 offset-lg-1 col-md-12">
        <div class="img-container">
            <img src="img/braden.jpeg"/>
        </div>
    </div>

    <!-- ABOUT ME -->
    <div class="col-lg-5 col-md-12 section">
        <div class="header">About Me<div class="line"></div></div>

        <div class="item">
            <div class="desc">I'm a Computer Science and Mathematics student at Duke University,
                currently taking part in research on more efficient methods of generalization and
                environment modeling in the reinforcement learning setting. I work part time as a machine
                learning engineer at Decipher Technology Studios in Alexandria, VA.
                <br/><br/>
            </div>
        </div>

        <div class="item">
            <div class="desc"><a href="https://computable.ai/">Computable AI: a machine intelligence blog</a></div>
            <br/>
            <div class="desc">Email: <a href="mailto:bch29@duke.edu">bch29@duke.edu</a></div>
            <!-- <br/> -->
            <div class="desc">GitHub: <a href="https://github.com/bchoagland">BCHoagland</a></div>
            <br/>
            <div class="desc"><a href="files/hoagland_CV.pdf">Curriculum Vitae</a></div>
        </div>
    </div>
</div>

<div class="row">
    <!-- NOTES -->
    <div class="col-lg-5 offset-lg-1 col-md-12 section">
        <div class="header">Misc. Notes<div class="line"></div></div>

        <div class="item">
            <div class="name"><a href="notes/kl">KL Divergence</a></div>
            <div class="desc">A non-symmetric probability distribution comparison</div>
        </div>

        <div class="item">
            <div class="name"><a href="notes/expected-value">Expected Value</a></div>
            <div class="desc">Formalizing intuitions about expected values</div>
        </div>

        <div class="item">
            <div class="name"><a href="notes/series-expansions">Series Expansions</a></div>
            <div class="desc">Approximating arbitrary functions using derivatives</div>
        </div>

        <div class="item">
            <div class="name"><a href="notes/lagrange">The Lagrangian</a></div>
            <div class="desc">Solving constrained optimization problems with Lagrange multipliers</div>
        </div>

        <div class="item">
            <div class="name"><a href="notes/dual-gd">Dual Gradient Descent</a></div>
            <div class="desc">Using the Lagrangian to perform gradient descent with a constraint</div>
        </div>
    </div>

    <!-- RL POSTS -->
    <div class="col-lg-5 col-md-12 section">
        <div class="header">RL Posts<div class="line"></div></div>

        <div class="item">
            <div class="name"><a href="posts/rl-intro">Intro to RL</a></div>
            <div class="desc">An introduction to the reinforcement learning setting, common themes in RL, and algorithm design</div>
        </div>

        <div class="item">
            <div class="name"><a href="posts/pg">Policy Gradients</a></div>
            <div class="desc">Derivation of and enhancements to the vanilla policy gradient</div>
        </div>

        <div class="item">
            <div class="name"><a href="posts/ac">Actor-Critic</a></div>
            <div class="desc">Strengthening the policy gradient with advantage approximation and value fitting</div>
        </div>

        <div class="item">
            <div class="name"><a href="posts/policy-free">Policy-Free Learning</a></div>
            <div class="desc">Using value estimates to guide action decisions instead of an explicit policy</div>
        </div>

        <div class="item">
            <div class="name"><a href="posts/deep-q">Deep Q-Learning</a></div>
            <div class="desc">Extending Q-Learning to work in complex environments</div>
        </div>

        <div class="item">
            <div class="name"><a href="posts/adv-pg">Advanced Policy Gradients</a></div>
            <div class="desc">Using surrogate functions and lots of math to develop stable, efficient policy gradient algorithms</div>
        </div>

        <div class="item">
            <div class="name"><a href="posts/max-entropy">Maximum Entropy RL</a></div>
            <div class="desc">Modifying the RL objective to maximize entropy and generate more robust policies</div>
        </div>
    </div>
</div>

</body>
</html>
