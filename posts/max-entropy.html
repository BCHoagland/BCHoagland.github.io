<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,700|Source+Sans+Pro:300,700" rel="stylesheet">
    <link rel="stylesheet" href="../styling/style.css">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <title>Max Entropy RL</title>
</head>
<body>

<div id="home-link"><a href="/">Home</a></div>

<div class="row">
    <div class="col-md-8">
        <div class="header">Maximum Entropy Reinforcement Learning</div>

        <p>
        With all the previous RL algorithms we've covered, we've tried to maximize expected return. All the different algorithms we have do exactly
        that, and their only defining features are how quickly we can go from a bad policy to a good one and how stable these policy updates are.
        Maximum entropy RL also maximizes expected return, but it modifies the maximization objective to ensure that whatever policy we end up with
        has explored sufficiently and is robust. Below are the normal RL objective and the maximum entropy RL objective
        \[
        \max_\pi \sum_t \mathbb{E}_{(s_t, a_t) \sim p_\pi} [r(s_t, a_t)] \\
        \max_\pi \sum_t \mathbb{E}_{(s_t, a_t) \sim p_\pi} [r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot |s_t))]
        \]
        As you can see, the max entropy RL objective only has one additional term, a scaled measure of the policy's entropy.
        </p>

        <p>
        </p>

        <div class="algo">
        <div>Soft Actor-Critic (SAC)</div>
        Initialize \(\theta_1\), \(\theta_2\), and \(\phi\) <br/>
        \( \bar{\theta}_1 \gets \theta_1, \bar{\theta}_2 \gets \theta_2 \) <br/>
        Initialize replay buffer \(\mathcal{D}\) <br/><br/>

        Repeat: <br/>
            <div>
            For each environment step: <br/>
                <div>
                \( a \sim \pi_\phi \) <br/>
                Observe \( s', r \) <br/>
                Store (\(s,a,r,s'\)) in \(\mathcal{D}\) <br/><br/>
                </div>
            For \(K\) epochs: <br/>
                <div>
                \( \theta_i \gets \theta_i - \lambda_Q \hat{\nabla}_{\theta_i} J_Q(\theta_i) \), for \(i \in {1, 2} \) <br/>
                \( \phi \gets \phi - \lambda_\pi \hat{\nabla}_{\phi} J_\pi(\phi) \) <br/>
                \( \alpha \gets \alpha - \lambda \hat{\nabla}_{\alpha} J(\alpha) \) <br/><br/>

                \( \bar{\theta}_i \gets \tau\bar{\theta}_i + (1 - \tau)\theta_i \)
                </div>
            </div>
        </div>
    </div>
</div>

</body>
</html>
