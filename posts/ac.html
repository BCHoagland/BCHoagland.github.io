<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,700|Source+Sans+Pro:300,700" rel="stylesheet">
    <link rel="stylesheet" href="../styling/style.css">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <title>Actor-Critic Methods</title>
</head>
<body>

<div id="home-link"><a href="/">Home</a></div>

<div class="row">
    <div class="col-md-8">
        <div class="header">Actor-Critic Methods</div>

        <p>
        Coming from the policy gradient, actor-critic algorithms can be seen as a natural bridge to a more complex algorithmic space. They offer
        more flexibility and greater decision power, and they are the basis of the current state-of-the-art policy optimization algorithms. The
        general idea with actor-critic algorithms is that we have two networks: one to choose actions and one to estimate the values of states the
        agent observes and actions it takes. As our critic trains, it feeds the actor more reliable ratings, which in turn creates better-performing
        policies.
        </p>

        <p>
        Actor-critic algorithms don't require the use of a policy gradient to update the actor, but they are almost always implemented this way in some
        form or another. They also allow for online learning, which means we can perform updates during an episode instead of waiting for termination.
        We'll start off with extending our policy gradient algorithm to use an actor-critic architecture, and then we'll see how online learning becomes
        possible.
        </p>

        <p>
        To begin, it's important to understand why having a separate critic network helps in the first place. With our vanilla policy gradient, our
        performance measure was always a reward observed by interacting with the environment. What this means, though, is that we're really just sampling
        rewards. For any given state, the same action could produce very different returns based on stochastic rewards and action choices throughout
        the remainder of the episode. Thus there will be extra variance introduced into our estimates by only using the rewards we observe directly.
        The role of our new critic network is to estimate the <i>expected</i> return we'll receive from a state-action pair so that we're working
        with less variable values when computing our gradient estimates.
        </p>

        <p>
        Using a critic is a good idea no matter how you look at it, but the question is how we'll do it. A solid strategy could be to estimate
        the advantage since this inherently incorporates a baseline, and this is in fact what state-of-the-art policy optimization algorithms do.
        The advantage function can be complicated, though. It involves both \( Q^\pi \) and \( V^\pi \) and requires both state and action as inputs.
        We could figure all this out, but it would be much easier to use a single, simpler neural network that takes only one input. It turns out we
        can approximate the advantage using just \( V^\pi \) and a single sample from the environment
        \[
        \begin{align*}
        Q^\pi(s_t, a_t) &= r(s_t, a_t) + \gamma\mathbb{E}[V^\pi(s_{t+1})] \\
        &\approx r(s_t, a_t) + \gamma V^\pi(s_{t+1}) \\ \\
        A^\pi(s_t, a_t) &= Q^\pi(s_t, a_t) - V^\pi(s_t) \\
        &\approx r(s_t, a_t) + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)
        \end{align*}
        \]
        After a single step in our environment, we have all the information we need to calculate the advantage for that step (we only need \(s_t\),
        \(a_t\), \(s_{t+1}\)).
        </p>

        <p>
        We know we can get the data we need, but now we need to figure out how to get an estimate for \(V^\pi\). This can actually be very
        straightforward. If we're okay with waiting until the end of an episode, we can just calculate rewards-to-go once the episode has
        terminated and then run supervised regression on our value estimate using those returns
        \[
        y_{i,t} = \hat{R}_{i,t} = \sum_{t'=t}^T r(s_{i,t'}, a_{i,t'}) \\
        \mathcal{L}(\phi) = \frac{1}{2} \sum_i \| V^\pi_\phi(s_i) - y_i \|^2 \\
        \phi \gets \arg\min\limits_\phi \mathcal{L}(\phi)
        \]
        This is all we need to make the policy gradient into an actor-critic algorithm. Calculating rewards-to-go means that we still have to wait
        until the end of each episode, but now we at least have an algorithm that works with two networks
        </p>

        <div class="algo">
        <div>Batch Actor-Critic</div>
        Repeat:
            <div>
            Sample \( \{s_i, a_i\} \) using \( \pi_\theta(a|s) \) <br/>
            Fit \( V^\pi_\phi \) using sampled returns <br/>
            \( A_i \gets r(s_i, a_i) + \gamma V^\pi_\phi(s'_i) - V^\pi_\phi(s_i) \) <br/>
            \( \theta \gets \theta + \alpha\big(\sum_i \nabla_\theta\log\pi_\theta(a_i|s_i) A_i\big) \)
            </div>
        </div>

        <p>
        In order to get to online learning, we'll have to introduce some bias into our regression targets. We can express a single target as
        \( y_{i,t} = r(s_{i,t}, a_{i,t}) + V^\pi_\phi(s_{i,t+1}) \). This is itself based on \( V^\pi_\phi \) (which is what we're trying to
        improve with the regression in the first place), but can actually lower variance once our estimator gets good enough. We're purposefully
        trading increased bias for lower variance, and this is usually worth it.
        </p>

        <p>
        Another important thing to note is that online learning requires the use of a discount factor. For the batch actor-critic algorithm, we could
        have left out the discount factor with no negative repucussions. Our returns would have been bounded because the episodes were guaranteed to
        terminate. This guarantee does not exist for online learning, however, since we could very well be working in an infinite-horizon environment.
        In order to make sure that our value estimator can actually converge to something, we need to include a discount factor in our return definition.
        </p>

        <div class="algo">
        <div>Online Actor-Critic</div>
        Repeat:
            <div>
            Take \( a \sim \pi_\theta(a|s) \), get \( (s, a, r, s') \) <br/>
            Update \( V^\pi_\phi \) using target \( r + \gamma V^\pi_\phi(s') \) <br/>
            \( A \gets r(s, a) + \gamma V^\pi_\phi(s') - V^\pi_\phi(s) \) <br/>
            \( \theta \gets \theta + \alpha\big(\nabla_\theta\log\pi_\theta(a|s) A\big) \)
            </div>
        </div>

        <p>
        We have a working online learning algorithm, but we can go one step further to make this closer to state-of-the-art. In order to do so,
        we need to take into account how many steps we're looking in the future for our value estimates before bootstrapping. In the above algorithm,
        we take one step and then estimate. We could also have taken 2 steps and then estimated, though, or even 3 or 4 or... The main idea here is that
        we can take n-steps through our environment and then use our value estimator.
        </p>

        <p>
        In theory, all these expressions would be the same if our value estimator were perfect. In practice, though, we know that our value estimator
        will be really bad when training first starts. During this period of training, having a large n lookahead will make our estimates more accurate.
        Once our estimator is good, though, having a large n means that we're sampling lots of rewards instead of using our good estimate of expected
        reward. Thus once \(V^\pi_\phi\) is accurate, we'll want low n lookaheads. This means we'll have to choose some big n to start with and then
        anneal that down as training progresses, but this isn't ideal. It would be much nicer if we didn't have to worry about n. This is where
        Generalized Advantage Estimation (GAE) comes in.
        </p>

        <p>
        GAE weights together many different n-step returns, with the weights decaying exponentially so that returns with shorter lookaheads have much
        more influence on the final value (\( w_n \propto \lambda^{n-1} \)). This allows larger n-returns to have some influence on what our
        advantage estimate ends up being without their larger variance having much of an effect later on in training. We can define an n-step
        advantage and then use many of those, weighted together, to form our final GAE advantage estimate
        \[
        \begin{align*}
        A^\pi_n &= \sum_{t'=t}^{t+n} \gamma^{t'-t} r(s_{t'}, a_{t'}) + \gamma^n V^\pi_\phi(s_{t+n}) - V^\pi_\phi(s_t) \\
        A^\pi_{GAE}(s_t, a_t) &= \sum_{t'=t}^\infty w_n(A^\pi_n(s_t, a_t))
        \end{align*}
        \]
        This can be simplified to make it easier to calculate in an actual program. Since this is a telescoping series, we can cancel out some terms
        and be left with
        \[
        \begin{align*}
        A^\pi_{GAE}(s_t, a_t) &= \sum_{t'=t}^\infty (\gamma\lambda)^{t'-t} \delta_{t'} \\
        \delta_{t'} &= r(s_{t'}, a_{t'}) + \gamma V^\pi_\phi(s_{t'+1}) - V^\pi_\phi(s_{t'})
        \end{align*}
        \]
        In practice we obviously can't sum up through infinity, so we'll just go a reasonable distance into the future and compute the advantage
        from there. Some of the current state-of-the-art algorithms are based on sampling small trajectories instead of individual observations, so
        this usually allows for a natural cutoff when using GAE for advantage estimation.
        </p>
    </div>
</div>

</body>
</html>
