<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,700|Source+Sans+Pro:300,700" rel="stylesheet">
    <link rel="stylesheet" href="../styling/style.css">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <title>Optimal Control Example</title>
</head>
<body>

<div class="row">
    <div class="col-md-8">
        <div class="header">Finding Zero in a Very Roundabout Way</div>

        <p>
        Let's say that you're operating an engine-powered cart on a 1-D track. You can go backwards or forwards, and you're trying to reach a certain point on the track. You want to get there as quickly as possible, but you also need to conserve fuel. This problem is all about finding the optimal trade-off between going quickly towards your goal and moving slowly to conserve fuel. So what do you do? Mathematically, what's the optimal way of maneuvering your cart?
        </p>

        <div class="subheader">Problem Formulation</div>
        <p>
        The first thing we need to do is formalize what's going on. Let's denote our position ("state") on the line by \(s \in \mathbb{R}\). We can denote the velocity of our cart (the "action" we're performing) by \(a \in \mathbb{R}\). Assuming that friction and air resistance aren't issues, we have a simple update rule for our new state \(s'\) after an infinitesimal amount of time \(dt\).
        \[
        s' = s + a \,dt
        \]
        As it'll be helpful later on, the time derivative of this update rule is clearly \(\dot{s} = a\), which we'll denote with the function \(f(s,a) = a\). The last thing we need to formalize is our objective. We want to get close to our goal and not go too fast (to spend fuel more efficiently). Without loss of generality, we can assume that our goal is 0. In this case, we can form a cost function \(c(s,a) = s^2 + a^2\). This penalizes large actions (i.e. moving quickly) and large states (i.e. states far from zero).
        </p>

        <div class="subheader">An Ugly Solution</div>
        <p>
        To solve this, I'll make use of the Hamilton-Jacobi-Bellman Equation. See <a href="/notes/hjb.html">this post</a> for a derivation of the equation, but I'll restate it here
        \[
        \dot{V}(t,s) + \min_a \left\{ c(t,s,a) + f(t,s,a) \cdot \partial_s V(t,s) \right\} = 0
        \]
        where \(V\) is some magical function that tells us how much future cost we'll incur if we act optimally from here on out. In our particular formulation, we didn't incorporate time, so this reduces to
        \[
        \min_a \left\{ c(s,a) + f(s,a) \cdot \partial_s V(s) \right\} = 0
        \]
        and we can subsitute in our specific functions to make this equation less abstract.
        \[
        \min_a \left\{ s^2 + a^2 + a \cdot \partial_s V(s) \right\} = 0
        \]
        To find out what the optimal action is (the \(a\) that minimizes the above equation), I'll introduce the assumption that the above equation is differentiable. Since we're working with a smooth cost and smooth dynamics, I think this is more than reasonable. Under this assumption, the minimizing \(a\) can be found using some basic calculus.
        </p>

        <p>
        First we take the derivative of our equation w.r.t. \(a\), set it equal to 0, and solve for \(a\) in terms of whatever's left. If we do this, we get
        \[
        \begin{align*}
        2a + \partial_s V &= 0 \\
        a &= -\frac{\partial_s V}{2}
        \end{align*}
        \]
        We can use substitute this into in our original equation to solve for \(\partial_s V\). Since we're acting optimally now, the minimum goes away, making this just a matter of algebra.
        \[
        \begin{align*}
        s^2 + \left( -\frac{\partial_s V}{2} \right)^2 - \frac{\partial_s V}{2} \cdot \partial_s V &= 0 \\
        s^2 + \frac{(\partial_s V)^2}{4} - \frac{(\partial_s V)^2}{2} &= 0 \\
        s^2 - \frac{(\partial_s V)^2}{4} &= 0 \\
        \partial_s V = \pm 2 s
        \end{align*}
        \]
        Plugging this into our formula for the optimal action gives us a nice closed form solution
        \[
        \begin{align*}
        a &= \pm s \\
        \end{align*}
        \]
        To determine whether we want the positive or negative answer, we could do some more math or just use common sense (which is what I'll do). If we're at state 100, moving 100 to the right is obviously stupid. Moving 100 to the <i>left</i>, however, immediately gets us to our goal. The same reasoning (but with flipped directions) holds for negative states, so our optimal action is
        \[
        a = -s
        \]
        </p>

        <p>
        All that math... just to find out that the optimal action is incredibly simple. Just move toward zero, with speed equal to how far away you are from it. It may be natural to wonder if this is always the case. What if the cost uses the exponent \(n\) instead of 2? What if there are coefficients? Personally, I don't want to type that math out again, especially not when everything is generalized (I already wrote it out by hand, and it wasn't fun). Luckily for me, though, we can find a simpler way of solving this problem. It still uses the HJB, but it completely sidesteps computing \(\partial_s V\). With this method, we can very quickly solve for the optimal action when our environment is generalized and even slightly more complex.
        </p>

        <div class="subheader">A New Method</div>
        <p>
        We'll start off with the time-independent HJB again.
        \[
        \min_a \left\{ c(s,a) + f(s,a) \cdot \partial_s V(s) \right\} = 0
        \]
        Under the assumption that this is a smooth function when acting optimally, we can solve the action by setting its action derivative to 0. This gives us two constraints
        \[
        \begin{align*}
        c(s,a) + f(s,a) \cdot \partial_s V(s) &= 0 \\
        \partial_a c(s,a) + \partial_a f(s,a) \cdot \partial_s V(s) &= 0
        \end{align*}
        \]
        where in both of these equations, \(a\) represents not just any action, but the optimal one. Solving for \(\partial_s V\) in both and then setting these expressions equal to each other gives us the nice relationship
        \[
        \partial_a c \cdot f = \partial_a f \cdot c
        \]
        So any otimal \(a\) must make this relationship true. Let's try it out with a more general version of the cart-on-a-track problem.
        </p>

        <p>
        Let \(c(s,a) = \alpha_1 s^n + \alpha_2 a^n\), where the Greek letters are real, positive coefficients. Let's also generalize the dynamics to be \(f(s,a) = \beta a^m\). (Why? Because we can!) Using the relationship we just derived, we're only a few steps away from knowing the optimal action.
        \[
        \begin{align*}
        \partial_a c \cdot f &= \partial_a f \cdot c \\
        (\alpha_2 n a^{n-1}) (\beta a^m) &= (\beta m a^{m-1}) (\alpha_1 s^n + \alpha_2 a^n) \\
        \alpha_2 {\color{blue}\beta} n a^{n+m-1} &= \alpha_1 {\color{blue}\beta} m s^n a^{m-1} + \alpha_2 {\color{blue}\beta} m a^{n+m-1} \\
        (n-m) \alpha_2 a^{n+m-1} &= \alpha_1 m s^n a^{m-1} \\
        a^n &= \frac{\alpha_1 m s^n}{\alpha_2 (n-m)} \\
        a &= \pm s \left( \frac{\alpha_1 m}{\alpha_2 (n-m)} \right)^{1/n}
        \end{align*}
        \]
        Beautiful :)
        </p>

        <p>
        We got two answers there, one positive and one negative. As before, we can reason that the negative one is the only actual solution to our problem. Before finishing up, I want to point out two things about this final formula that surprised me. 1) no matter what \(n\) or \(m\) we use, the optimal action is only ever a linear function of the state. The complexity of the optimal action never changes, just the coefficient next to it! 2) the coefficient for the dynamics doesn't actually affect the optimal action. Our action could be magnified or reduced in magnitude and the overall strategy for getting to our goal doesn't change. I found that pretty fascinating.
        </p>

        <p>
        With this technique, we can make our cost and dynamics increasingly complex and general without requiring much extra effort on our part to determine the optimal action. We could add in a "drift" term to our dynamics to make the cart repel from the goal (because why not?), or maybe add in additional terms to mimic friction or maybe even a sloped track.
        </p>
    </div>
</div>

</body>
</html>